{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfaed1c6",
   "metadata": {},
   "source": [
    "### ECMC Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, seaborn as sns, tensorflow as tf, xgboost as xgb, warnings, os, sys, time, datetime, gc\n",
    "from sklearn.model_selection import train_test_split as tts, StratifiedKFold as skf, GridSearchCV as gscv, cross_val_score as cvs, validation_curve as vc, learning_curve as lc\n",
    "from sklearn.preprocessing import StandardScaler as ss, OneHotEncoder as ohe, LabelEncoder as le, PolynomialFeatures as pf, MinMaxScaler as mms, RobustScaler as rs, PowerTransformer as pt, QuantileTransformer as qt\n",
    "from sklearn.compose import ColumnTransformer as ct, make_column_selector as mcs, make_column_transformer as mct\n",
    "from sklearn.pipeline import Pipeline as pl, make_pipeline as mp\n",
    "from sklearn.linear_model import LogisticRegression as lr, Ridge as rdg, Lasso as lso, ElasticNet as en\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc, GradientBoostingClassifier as gbc, StackingClassifier as sc, VotingClassifier as vc_cls, ExtraTreesClassifier as etc, AdaBoostClassifier as abc\n",
    "from sklearn.svm import SVC as svc, LinearSVC as lsvc\n",
    "from sklearn.impute import SimpleImputer as si, KNNImputer as ki\n",
    "from sklearn.feature_selection import SelectKBest as skb, f_classif as fc, mutual_info_classif as mic, RFE as rfe, SelectFromModel as sfm\n",
    "from sklearn.metrics import classification_report as cr, confusion_matrix as cm, ConfusionMatrixDisplay as cmd, roc_curve as rc, auc as auc_fn, accuracy_score as acc, precision_score as ps, recall_score as rs_score, f1_score as f1, log_loss as ll, roc_auc_score as ras\n",
    "from sklearn.decomposition import PCA as pca, TruncatedSVD as tsvd\n",
    "from sklearn.manifold import TSNE as tsne\n",
    "from imblearn.over_sampling import SMOTE as sm, ADASYN as ad, BorderlineSMOTE as bsm, SVMSMOTE as ssm\n",
    "from imblearn.under_sampling import RandomUnderSampler as rus, EditedNearestNeighbours as enn, TomekLinks as tl\n",
    "from imblearn.combine import SMOTETomek as smt, SMOTEENN as sen\n",
    "from imblearn.pipeline import Pipeline as ipl\n",
    "from tensorflow.keras import layers as lyr, models as mdl, optimizers as opt, regularizers as reg, callbacks as cb, utils as ku\n",
    "from tensorflow.keras.callbacks import EarlyStopping as es, ReduceLROnPlateau as rlp, ModelCheckpoint as mcp\n",
    "import matplotlib.pyplot as plt, matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap as lscm, ListedColormap as licm\n",
    "import joblib, xlsxwriter, pickle, json, hashlib, itertools, functools, collections, copy\n",
    "from scipy import stats as sp_stats\n",
    "from scipy.spatial.distance import pdist as pd_dist, squareform as sq_form\n",
    "from scipy.cluster.hierarchy import dendrogram as dend, linkage as link\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({k: v for k, v in [('font.family', 'Arial'), ('font.size', 11), ('font.weight', 'bold'), ('axes.linewidth', 1.2), ('axes.labelsize', 14), ('axes.labelweight', 'bold'), ('axes.titlesize', 16), ('axes.titleweight', 'bold'), ('xtick.major.width', 1.2), ('ytick.major.width', 1.2), ('xtick.major.size', 5), ('ytick.major.size', 5), ('xtick.labelsize', 12), ('ytick.labelsize', 12), ('legend.frameon', False), ('legend.fontsize', 11), ('figure.dpi', 600), ('savefig.dpi', 600), ('savefig.bbox', 'tight'), ('savefig.pad_inches', 0.1)]})\n",
    "\n",
    "fp_base = 'C:\\\\Users\\\\anisi\\\\OneDrive - Colorado School of Mines\\\\Documents\\\\PhD\\\\PhD Research\\\\ECMC\\\\Spills_Data.xlsx'\n",
    "fp_main = os.path.join(fp_base, 'Spills_Data.xlsx')\n",
    "dt_hash = hashlib.md5(fp_main.encode()).hexdigest()[:8]\n",
    "start_time = time.time()\n",
    "\n",
    "df_raw = pd.read_excel(fp_main)\n",
    "df_backup = df_raw.copy(deep=True)\n",
    "df_work = df_raw.copy(deep=True)\n",
    "\n",
    "# Data structure analysis\n",
    "dt_info = {'shape': df_work.shape, 'columns': list(df_work.columns), 'dtypes': df_work.dtypes.to_dict(), 'missing': df_work.isnull().sum().to_dict()}\n",
    "numeric_cols = df_work.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_work.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = df_work.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "tg_primary = 'Failure_Score'\n",
    "tg_secondary = 'Failure_Category'\n",
    "ft_base = ['Flowline_Age', 'MAXOPPRESSURE', 'oil_spilled', 'PW_spilled', 'cond_spilled', 'other_spilled', 'MAXOD', 'LAT', 'LONG']\n",
    "ft_encoded = ['WeatherConditions_Freq_Encoded', 'Root_Cause_Freq_Encoded', 'Spill_Type_Target_Encoded', 'Typeoffluidtrans_Encoded', 'Pipematerial_Freq_Encoded', 'Basin_Freq_Encoded', 'Flowlinetype_Freq_Encoded']\n",
    "ft_combined = ft_base + ft_encoded\n",
    "\n",
    "categorical_mapping_config = [\n",
    "    ('Root Cause Type', 'Root_Cause_Freq_Encoded', 'frequency'),\n",
    "    ('TYPEOFFLUIDTRANS', 'Typeoffluidtrans_Encoded', 'frequency'),\n",
    "    ('PIPEMATERIAL', 'Pipematerial_Freq_Encoded', 'frequency'),\n",
    "    ('Basin', 'Basin_Freq_Encoded', 'frequency'),\n",
    "    ('FLOWLINETYPE', 'Flowlinetype_Freq_Encoded', 'frequency'),\n",
    "    ('Weather Conditions', 'WeatherConditions_Freq_Encoded', 'frequency'),\n",
    "    ('Spill Type_x', 'Spill_Type_Target_Encoded', 'target_mean')\n",
    "]\n",
    "\n",
    "def advanced_categorical_encoder(df_input, mapping_config):\n",
    "    df_output = df_input.copy()\n",
    "    encoding_stats = {}\n",
    "    \n",
    "    for original_col, encoded_col, encoding_type in mapping_config:\n",
    "        if original_col not in df_output.columns:\n",
    "            continue\n",
    "            \n",
    "        if encoding_type == 'frequency':\n",
    "            freq_dict = df_output[original_col].value_counts().to_dict()\n",
    "            df_output[encoded_col] = df_output[original_col].map(freq_dict)\n",
    "            encoding_stats[encoded_col] = {'type': 'frequency', 'unique_values': len(freq_dict), 'min_freq': min(freq_dict.values()), 'max_freq': max(freq_dict.values())}\n",
    "            \n",
    "        elif encoding_type == 'target_mean':\n",
    "            if tg_primary in df_output.columns:\n",
    "                target_mean_dict = df_output.groupby(original_col)[tg_primary].mean(numeric_only=True).to_dict()\n",
    "                df_output[encoded_col] = df_output[original_col].map(target_mean_dict)\n",
    "                encoding_stats[encoded_col] = {'type': 'target_mean', 'unique_values': len(target_mean_dict)}\n",
    "    \n",
    "    return df_output, encoding_stats\n",
    "\n",
    "df_work, encoding_statistics = advanced_categorical_encoder(df_work, categorical_mapping_config)\n",
    "\n",
    "def weather_categorization_advanced(condition_str):\n",
    "    if pd.isna(condition_str):\n",
    "        return 'Unknown'\n",
    "    condition_lower = str(condition_str).lower()\n",
    "    clear_indicators = ['clear', 'sunny', 'bright', 'cloudless']\n",
    "    rain_indicators = ['rain', 'wet', 'drizzle', 'shower', 'precipitation']\n",
    "    snow_indicators = ['snow', 'freezing', 'frost', 'ice', 'blizzard']\n",
    "    cloud_indicators = ['cloudy', 'overcast', 'hazy', 'foggy', 'misty']\n",
    "    wind_indicators = ['wind', 'gusty', 'breezy', 'storm']\n",
    "    \n",
    "    condition_scores = {\n",
    "        'Clear': sum(1 for indicator in clear_indicators if indicator in condition_lower),\n",
    "        'Rainy': sum(1 for indicator in rain_indicators if indicator in condition_lower),\n",
    "        'Snowy': sum(1 for indicator in snow_indicators if indicator in condition_lower),\n",
    "        'Cloudy': sum(1 for indicator in cloud_indicators if indicator in condition_lower),\n",
    "        'Windy': sum(1 for indicator in wind_indicators if indicator in condition_lower)\n",
    "    }\n",
    "    \n",
    "    max_score_category = max(condition_scores, key=condition_scores.get)\n",
    "    return max_score_category if condition_scores[max_score_category] > 0 else 'Other'\n",
    "\n",
    "df_work['Weather Conditions'] = df_work['Weather Conditions'].apply(weather_categorization_advanced)\n",
    "\n",
    "interaction_feature_configs = [\n",
    "    ('Flowline_Age', 'PIPEMATERIAL', 'Age_Material_Interaction', 'string_concat'),\n",
    "    ('MAXOPPRESSURE', 'TYPEOFFLUIDTRANS', 'Pressure_Type_Interaction', 'string_concat'),\n",
    "    ('Flowline_Age', 'MAXOPPRESSURE', 'Age_Pressure_Product', 'multiply'),\n",
    "    ('oil_spilled', 'PW_spilled', 'Total_Liquid_Spilled', 'add'),\n",
    "    ('LAT', 'LONG', 'Geographic_Distance', 'euclidean')\n",
    "]\n",
    "\n",
    "def create_interaction_features(df_input, feature_configs):\n",
    "    df_output = df_input.copy()\n",
    "    interaction_stats = {}\n",
    "    \n",
    "    for feat1, feat2, new_feat, operation in feature_configs:\n",
    "        if feat1 not in df_output.columns or feat2 not in df_output.columns:\n",
    "            continue\n",
    "            \n",
    "        if operation == 'string_concat':\n",
    "            df_output[new_feat] = df_output[feat1].astype(str) + '_' + df_output[feat2].astype(str)\n",
    "        elif operation == 'multiply':\n",
    "            df_output[new_feat] = df_output[feat1] * df_output[feat2]\n",
    "        elif operation == 'add':\n",
    "            df_output[new_feat] = df_output[feat1] + df_output[feat2]\n",
    "        elif operation == 'euclidean':\n",
    "            df_output[new_feat] = np.sqrt((df_output[feat1] - df_output[feat1].mean())**2 + (df_output[feat2] - df_output[feat2].mean())**2)\n",
    "        \n",
    "        interaction_stats[new_feat] = {'operation': operation, 'source_features': [feat1, feat2]}\n",
    "    \n",
    "    return df_output, interaction_stats\n",
    "\n",
    "df_work, interaction_statistics = create_interaction_features(df_work, interaction_feature_configs)\n",
    "\n",
    "binning_configurations = [\n",
    "    ('MAXOPPRESSURE', 'MAXOPPRESSURE_BIN', 5, ['Very Low', 'Low', 'Medium', 'High', 'Very High']),\n",
    "    ('Flowline_Age', 'Flowline_Age_BIN', 5, ['Very New', 'New', 'Middle-Aged', 'Old', 'Very Old']),\n",
    "    ('MAXOD', 'MAXOD_BIN', 4, ['Small', 'Medium', 'Large', 'Very Large']),\n",
    "    ('oil_spilled', 'Oil_Spill_BIN', 3, ['Low', 'Medium', 'High'])\n",
    "]\n",
    "\n",
    "def create_binned_features(df_input, binning_configs):\n",
    "    df_output = df_input.copy()\n",
    "    binning_stats = {}\n",
    "    \n",
    "    for col, new_col, n_bins, labels in binning_configs:\n",
    "        if col not in df_output.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df_output[new_col] = pd.cut(df_output[col], bins=n_bins, labels=labels)\n",
    "            binning_stats[new_col] = {'original_column': col, 'bins': n_bins, 'labels': labels}\n",
    "        except Exception as e:\n",
    "            print(f\"Binning failed for {col}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return df_output, binning_stats\n",
    "\n",
    "df_work, binning_statistics = create_binned_features(df_work, binning_configurations)\n",
    "\n",
    "categorical_columns_extended = ['Flowline_Age_BIN', 'Weather Conditions', 'PIPEMATERIAL', 'TYPEOFFLUIDTRANS', 'MAXOPPRESSURE_BIN', 'Root Cause Type', 'Basin', 'Spill Type_x', 'Age_Material_Interaction', 'Pressure_Type_Interaction']\n",
    "for col in categorical_columns_extended:\n",
    "    if col in df_work.columns:\n",
    "        df_work[col] = df_work[col].astype(str)\n",
    "\n",
    "numerical_columns_extended = ['MAXOD', 'Age_Pressure_Product', 'Total_Liquid_Spilled', 'Geographic_Distance']\n",
    "for col in numerical_columns_extended:\n",
    "    if col in df_work.columns:\n",
    "        df_work[col] = pd.to_numeric(df_work[col], errors='coerce')\n",
    "\n",
    "failure_categorization_schemes = {\n",
    "    'conservative': (0.10, 0.80),\n",
    "    'balanced': (0.20, 0.70),\n",
    "    'aggressive': (0.35, 0.65),\n",
    "    'moderate_low': (0.15, 0.75),\n",
    "    'moderate_high': (0.25, 0.75)\n",
    "}\n",
    "\n",
    "def create_multiple_categorizations(df_input, target_col, schemes):\n",
    "    df_output = df_input.copy()\n",
    "    categorization_stats = {}\n",
    "    \n",
    "    for scheme_name, (low_thresh, high_thresh) in schemes.items():\n",
    "        low_val = df_output[target_col].quantile(low_thresh)\n",
    "        high_val = df_output[target_col].quantile(high_thresh)\n",
    "        \n",
    "        def categorize_score(score):\n",
    "            if pd.isna(score):\n",
    "                return 'Unknown'\n",
    "            return 'Low' if score < low_val else ('Medium' if score < high_val else 'High')\n",
    "        \n",
    "        category_col = f'Failure_Category_{scheme_name}'\n",
    "        df_output[category_col] = df_output[target_col].apply(categorize_score)\n",
    "        \n",
    "        categorization_stats[category_col] = {\n",
    "            'scheme': scheme_name,\n",
    "            'thresholds': (low_thresh, high_thresh),\n",
    "            'values': (low_val, high_val),\n",
    "            'distribution': df_output[category_col].value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    return df_output, categorization_stats\n",
    "\n",
    "df_work, categorization_stats = create_multiple_categorizations(df_work, tg_primary, failure_categorization_schemes)\n",
    "\n",
    "feature_sets_config = {\n",
    "    'basic': ft_combined,\n",
    "    'extended': ft_combined + ['Age_Material_Interaction', 'Pressure_Type_Interaction', 'Age_Pressure_Product'],\n",
    "    'binned': ['Flowline_Age_BIN', 'Weather Conditions', 'PIPEMATERIAL', 'TYPEOFFLUIDTRANS', 'MAXOPPRESSURE_BIN', 'MAXOD', 'Root Cause Type', 'Basin', 'Spill Type_x', 'Age_Material_Interaction', 'Pressure_Type_Interaction'],\n",
    "    'hybrid': ft_combined + ['Flowline_Age_BIN', 'MAXOPPRESSURE_BIN', 'Age_Material_Interaction', 'Total_Liquid_Spilled'],\n",
    "    'comprehensive': ft_combined + ['Age_Material_Interaction', 'Pressure_Type_Interaction', 'Age_Pressure_Product', 'Total_Liquid_Spilled', 'Geographic_Distance', 'Flowline_Age_BIN', 'MAXOPPRESSURE_BIN']\n",
    "}\n",
    "\n",
    "preprocessing_pipeline_configs = {\n",
    "    'lr_standard': {\n",
    "        'numerical': pl([('imputer', si(strategy='mean')), ('scaler', ss())]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'lr_robust': {\n",
    "        'numerical': pl([('imputer', ki(n_neighbors=5)), ('scaler', rs())]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'rf_basic': {\n",
    "        'numerical': pl([('imputer', si(strategy='median')), ('scaler', ss())]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(drop='first', handle_unknown='ignore'))])\n",
    "    },\n",
    "    'rf_advanced': {\n",
    "        'numerical': pl([('imputer', ki(n_neighbors=3)), ('scaler', rs()), ('pca', pca(n_components=0.95))]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(drop='first', handle_unknown='ignore'))])\n",
    "    },\n",
    "    'ann_deep': {\n",
    "        'numerical': pl([('imputer', si(strategy='mean')), ('scaler', mms()), ('power', pt())]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'svm_polynomial': {\n",
    "        'numerical': pl([('imputer', ki(n_neighbors=5)), ('scaler', ss()), ('poly', pf(degree=2, interaction_only=True))]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'svm_kernel': {\n",
    "        'numerical': pl([('imputer', si(strategy='median')), ('scaler', rs()), ('quantile', qt(n_quantiles=100))]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'ensemble_stacking': {\n",
    "        'numerical': pl([('imputer', ki(n_neighbors=7)), ('scaler', ss()), ('feature_select', skb(fc, k=20))]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    },\n",
    "    'ensemble_voting': {\n",
    "        'numerical': pl([('imputer', si(strategy='mean')), ('scaler', rs()), ('pca', pca(n_components=15))]),\n",
    "        'categorical': pl([('imputer', si(strategy='most_frequent')), ('encoder', ohe(handle_unknown='ignore'))])\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_preprocessor(config_name, numerical_features, categorical_features):\n",
    "    if config_name not in preprocessing_pipeline_configs:\n",
    "        config_name = 'lr_standard'\n",
    "    \n",
    "    config = preprocessing_pipeline_configs[config_name]\n",
    "    return ct(transformers=[\n",
    "        ('num', config['numerical'], numerical_features),\n",
    "        ('cat', config['categorical'], categorical_features)\n",
    "    ])\n",
    "\n",
    "resampling_strategies = {\n",
    "    'smote_basic': sm(random_state=42),\n",
    "    'smote_borderline': bsm(random_state=42, kind='borderline-1'),\n",
    "    'smote_svm': ssm(random_state=42),\n",
    "    'adasyn': ad(random_state=42),\n",
    "    'smote_tomek': smt(random_state=42),\n",
    "    'smote_enn': sen(random_state=42),\n",
    "    'random_under': rus(random_state=42)\n",
    "}\n",
    "\n",
    "def apply_resampling(X, y, strategy_name):\n",
    "    if strategy_name not in resampling_strategies:\n",
    "        strategy_name = 'smote_basic'\n",
    "    \n",
    "    try:\n",
    "        resampler = resampling_strategies[strategy_name]\n",
    "        return resampler.fit_resample(X, y)\n",
    "    except Exception as e:\n",
    "        print(f\"Resampling {strategy_name} failed: {e}, using basic SMOTE\")\n",
    "        return sm(random_state=42).fit_resample(X, y)\n",
    "\n",
    "model_configurations = {\n",
    "    'lr_basic': {\n",
    "        'model': lr(random_state=42),\n",
    "        'params': {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']},\n",
    "        'features': 'basic',\n",
    "        'target': 'Failure_Category_conservative',\n",
    "        'preprocessor': 'lr_standard',\n",
    "        'resampling': 'smote_basic'\n",
    "    },\n",
    "    'lr_advanced': {\n",
    "        'model': lr(random_state=42, max_iter=1000),\n",
    "        'params': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2', 'elasticnet'], 'l1_ratio': [0.1, 0.5, 0.9], 'solver': ['saga']},\n",
    "        'features': 'extended',\n",
    "        'target': 'Failure_Category_balanced',\n",
    "        'preprocessor': 'lr_robust',\n",
    "        'resampling': 'smote_borderline'\n",
    "    },\n",
    "    'rf_basic': {\n",
    "        'model': rfc(random_state=42, class_weight='balanced'),\n",
    "        'params': {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20], 'min_samples_split': [2, 5, 10]},\n",
    "        'features': 'binned',\n",
    "        'target': 'Failure_Category_balanced',\n",
    "        'preprocessor': 'rf_basic',\n",
    "        'resampling': 'smote_basic'\n",
    "    },\n",
    "    'rf_advanced': {\n",
    "        'model': rfc(random_state=42, class_weight='balanced_subsample'),\n",
    "        'params': {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'max_features': ['sqrt', 'log2', None]},\n",
    "        'features': 'comprehensive',\n",
    "        'target': 'Failure_Category_moderate_low',\n",
    "        'preprocessor': 'rf_advanced',\n",
    "        'resampling': 'adasyn'\n",
    "    },\n",
    "    'svm_rbf': {\n",
    "        'model': svc(probability=True, kernel='rbf', random_state=42, class_weight='balanced'),\n",
    "        'params': {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]},\n",
    "        'features': 'basic',\n",
    "        'target': 'Failure_Category_conservative',\n",
    "        'preprocessor': 'svm_kernel',\n",
    "        'resampling': 'smote_svm'\n",
    "    },\n",
    "    'svm_poly': {\n",
    "        'model': svc(probability=True, kernel='poly', random_state=42, class_weight='balanced'),\n",
    "        'params': {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']},\n",
    "        'features': 'extended',\n",
    "        'target': 'Failure_Category_aggressive',\n",
    "        'preprocessor': 'svm_polynomial',\n",
    "        'resampling': 'smote_tomek'\n",
    "    },\n",
    "    'ann_shallow': {\n",
    "        'model': 'neural_network',\n",
    "        'architecture': [(64, 'relu'), (32, 'relu')],\n",
    "        'params': {'batch_size': [16, 32, 64], 'epochs': [50, 100, 150], 'learning_rate': [0.001, 0.01]},\n",
    "        'features': 'hybrid',\n",
    "        'target': 'Failure_Category_balanced',\n",
    "        'preprocessor': 'ann_deep',\n",
    "        'resampling': 'smote_basic'\n",
    "    },\n",
    "    'ann_deep': {\n",
    "        'model': 'neural_network',\n",
    "        'architecture': [(128, 'relu'), (64, 'relu'), (32, 'relu'), (16, 'relu')],\n",
    "        'params': {'batch_size': [32, 64], 'epochs': [100, 200], 'learning_rate': [0.0001, 0.001]},\n",
    "        'features': 'comprehensive',\n",
    "        'target': 'Failure_Category_moderate_high',\n",
    "        'preprocessor': 'ann_deep',\n",
    "        'resampling': 'adasyn'\n",
    "    }\n",
    "}\n",
    "\n",
    "ensemble_configurations = {\n",
    "    'voting_soft': {\n",
    "        'type': 'voting',\n",
    "        'base_models': ['lr_basic', 'rf_basic', 'svm_rbf'],\n",
    "        'voting': 'soft',\n",
    "        'features': 'extended',\n",
    "        'target': 'Failure_Category_balanced',\n",
    "        'preprocessor': 'ensemble_voting',\n",
    "        'resampling': 'smote_basic'\n",
    "    },\n",
    "    'voting_hard': {\n",
    "        'type': 'voting',\n",
    "        'base_models': ['lr_advanced', 'rf_advanced', 'svm_poly'],\n",
    "        'voting': 'hard',\n",
    "        'features': 'comprehensive',\n",
    "        'target': 'Failure_Category_moderate_low',\n",
    "        'preprocessor': 'ensemble_voting',\n",
    "        'resampling': 'adasyn'\n",
    "    },\n",
    "    'stacking_basic': {\n",
    "        'type': 'stacking',\n",
    "        'base_models': [\n",
    "            rfc(n_estimators=100, random_state=42),\n",
    "            gbc(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "            svc(probability=True, random_state=42)\n",
    "        ],\n",
    "        'meta_model': lr(random_state=42),\n",
    "        'features': 'hybrid',\n",
    "        'target': 'Failure_Category_balanced',\n",
    "        'preprocessor': 'ensemble_stacking',\n",
    "        'resampling': 'smote_tomek'\n",
    "    },\n",
    "    'stacking_advanced': {\n",
    "        'type': 'stacking',\n",
    "        'base_models': [\n",
    "            rfc(n_estimators=200, max_depth=10, random_state=42),\n",
    "            etc(n_estimators=200, max_depth=15, random_state=42),\n",
    "            gbc(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42),\n",
    "            xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "        ],\n",
    "        'meta_model': lr(C=10, random_state=42),\n",
    "        'features': 'comprehensive',\n",
    "        'target': 'Failure_Category_moderate_high',\n",
    "        'preprocessor': 'ensemble_stacking',\n",
    "        'resampling': 'smote_enn'\n",
    "    }\n",
    "}\n",
    "\n",
    "visualization_color_schemes = {\n",
    "    'publication': {\n",
    "        'colors': [\"#ffffff\", \"#e0e0e0\", \"#c0c0c0\", \"#909090\", \"#606060\", \"#303030\"],\n",
    "        'risk_colors': {'High': '#800000', 'Medium': '#F0A202', 'Low': '#0D47A1'},\n",
    "        'line_styles': {'High': '-', 'Medium': '--', 'Low': '-.'},\n",
    "        'line_widths': {'High': 3.0, 'Medium': 2.5, 'Low': 2.0}\n",
    "    },\n",
    "    'colorblind_friendly': {\n",
    "        'colors': [\"#ffffff\", \"#f0f0f0\", \"#d9d9d9\", \"#bdbdbd\", \"#969696\", \"#636363\"],\n",
    "        'risk_colors': {'High': '#e31a1c', 'Medium': '#ff7f00', 'Low': '#1f78b4'},\n",
    "        'line_styles': {'High': '-', 'Medium': '--', 'Low': '-.'},\n",
    "        'line_widths': {'High': 3.0, 'Medium': 2.5, 'Low': 2.0}\n",
    "    },\n",
    "    'grayscale': {\n",
    "        'colors': [\"#ffffff\", \"#f7f7f7\", \"#cccccc\", \"#969696\", \"#636363\", \"#252525\"],\n",
    "        'risk_colors': {'High': '#252525', 'Medium': '#636363', 'Low': '#969696'},\n",
    "        'line_styles': {'High': '-', 'Medium': '--', 'Low': '-.'},\n",
    "        'line_widths': {'High': 3.0, 'Medium': 2.5, 'Low': 2.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "ordered_classes = ['High', 'Medium', 'Low']\n",
    "\n",
    "def create_advanced_confusion_matrix(y_true, y_pred, model_name, color_scheme='publication'):\n",
    "    cm_matrix = cm(y_true, y_pred)\n",
    "    original_classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    \n",
    "    try:\n",
    "        indices = [np.where(original_classes == cls)[0][0] for cls in ordered_classes if cls in original_classes]\n",
    "        reordered_cm = cm_matrix[indices, :][:, indices] if len(indices) == len(ordered_classes) else cm_matrix\n",
    "        display_labels = [ordered_classes[i] for i in range(len(indices))] if len(indices) < len(ordered_classes) else ordered_classes\n",
    "    except:\n",
    "        reordered_cm = cm_matrix\n",
    "        display_labels = original_classes\n",
    "    \n",
    "    scheme = visualization_color_schemes[color_scheme]\n",
    "    cmap = lscm.from_list(\"publication_cmap\", scheme['colors'], N=256)\n",
    "    \n",
    "    fig_sizes = [(6, 5), (8, 6), (7, 5.5)]\n",
    "    for i, fig_size in enumerate(fig_sizes):\n",
    "        fig, ax = plt.subplots(figsize=fig_size, dpi=600)\n",
    "        \n",
    "        conf_display = cmd(confusion_matrix=reordered_cm, display_labels=display_labels)\n",
    "        conf_display.plot(ax=ax, cmap=cmap, colorbar=False, values_format='d', \n",
    "                         text_kw={\"weight\": \"bold\", \"fontsize\": 14, \"color\": \"black\"})\n",
    "        \n",
    "        ax.set_xlabel('Predicted label', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        ax.set_ylabel('True label', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        file_suffix = f\"_size{i+1}\" if i > 0 else \"\"\n",
    "        for ext in ['pdf', 'png', 'svg', 'tiff']:\n",
    "            filename = f\"confusion_matrix_{model_name}_{color_scheme}{file_suffix}.{ext}\"\n",
    "            if ext == 'tiff':\n",
    "                plt.savefig(filename, format=ext, dpi=600, bbox_inches='tight', pil_kwargs={\"compression\": \"lzw\"})\n",
    "            else:\n",
    "                plt.savefig(filename, format=ext, dpi=600, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_advanced_roc_curves(y_true, y_prob, model_obj, model_name, color_scheme='publication'):\n",
    "    scheme = visualization_color_schemes[color_scheme]\n",
    "    \n",
    "    if hasattr(model_obj, 'classes_'):\n",
    "        classes = model_obj.classes_\n",
    "    else:\n",
    "        classes = np.unique(y_true)\n",
    "    \n",
    "    class_indices = {cls: i for i, cls in enumerate(classes)}\n",
    "    \n",
    "    fig_configs = [\n",
    "        {'size': (6, 6), 'suffix': ''},\n",
    "        {'size': (8, 8), 'suffix': '_large'},\n",
    "        {'size': (10, 8), 'suffix': '_extra_large'}\n",
    "    ]\n",
    "    \n",
    "    for config in fig_configs:\n",
    "        fig, ax = plt.subplots(figsize=config['size'], dpi=600)\n",
    "        \n",
    "        workbook = xlsxwriter.Workbook(f'roc_data_{model_name}{config[\"suffix\"]}.xlsx')\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        row = 0\n",
    "        \n",
    "        for class_label in ordered_classes:\n",
    "            if class_label in class_indices and y_prob.shape[1] > class_indices[class_label]:\n",
    "                idx = class_indices[class_label]\n",
    "                fpr, tpr, _ = rc(y_true == class_label, y_prob[:, idx])\n",
    "                roc_auc = auc_fn(fpr, tpr)\n",
    "                \n",
    "                ax.plot(fpr, tpr, \n",
    "                       lw=scheme['line_widths'][class_label], \n",
    "                       color=scheme['risk_colors'][class_label],\n",
    "                       linestyle=scheme['line_styles'][class_label],\n",
    "                       label=f'{class_label} Risk (AUC = {roc_auc:.3f})',\n",
    "                       marker='o', markersize=2, markevery=max(1, len(fpr)//20))\n",
    "                \n",
    "                worksheet.write(row, 0, f'Class {class_label} FPR')\n",
    "                worksheet.write(row, 1, f'Class {class_label} TPR')\n",
    "                worksheet.write(row, 2, f'AUC = {roc_auc:.3f}')\n",
    "                for j in range(len(fpr)):\n",
    "                    worksheet.write(row + j + 1, 0, fpr[j])\n",
    "                    worksheet.write(row + j + 1, 1, tpr[j])\n",
    "                row += len(fpr) + 3\n",
    "        \n",
    "        workbook.close()\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], color='#FF0000', linestyle='--', lw=2.0, alpha=0.8, label='Random Chance')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "        \n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        risk_handles = [h for h, l in zip(handles, labels) if 'Random' not in l]\n",
    "        risk_labels = [l for l in labels if 'Random' not in l]\n",
    "        \n",
    "        order_mapping = {f'{cls} Risk (AUC = ': i for i, cls in enumerate(ordered_classes)}\n",
    "        sorted_indices = sorted(range(len(risk_labels)), \n",
    "                              key=lambda i: next((idx for prefix, idx in order_mapping.items() \n",
    "                                                 if risk_labels[i].startswith(prefix)), 999))\n",
    "        \n",
    "        risk_handles = [risk_handles[i] for i in sorted_indices]\n",
    "        risk_labels = [risk_labels[i] for i in sorted_indices]\n",
    "        \n",
    "        if 'Random Chance' in labels:\n",
    "            random_idx = labels.index('Random Chance')\n",
    "            risk_handles.append(handles[random_idx])\n",
    "            risk_labels.append('Random Chance')\n",
    "        \n",
    "        ax.legend(risk_handles, risk_labels, loc='lower right', fontsize=12, frameon=True, \n",
    "                 framealpha=0.9, edgecolor='black', facecolor='white')\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', width=1.5, length=6, labelsize=12)\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        for ext in ['pdf', 'png', 'svg', 'tiff']:\n",
    "            filename = f\"roc_curve_{model_name}_{color_scheme}{config['suffix']}.{ext}\"\n",
    "            if ext == 'tiff':\n",
    "                plt.savefig(filename, format=ext, dpi=600, bbox_inches='tight', pil_kwargs={\"compression\": \"lzw\"})\n",
    "            else:\n",
    "                plt.savefig(filename, format=ext, dpi=600, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_neural_network_model(input_dim, output_dim, architecture):\n",
    "    model = mdl.Sequential()\n",
    "    model.add(lyr.Dense(architecture[0][0], input_dim=input_dim, activation=architecture[0][1]))\n",
    "    model.add(lyr.Dropout(0.3))\n",
    "    \n",
    "    for layer_size, activation in architecture[1:]:\n",
    "        model.add(lyr.Dense(layer_size, activation=activation))\n",
    "        model.add(lyr.Dropout(0.2))\n",
    "    \n",
    "    model.add(lyr.Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "    optimizer_configs = [\n",
    "        opt.Adam(learning_rate=0.001),\n",
    "        opt.Adam(learning_rate=0.0001),\n",
    "        opt.RMSprop(learning_rate=0.001),\n",
    "        opt.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    ]\n",
    "    \n",
    "    model.compile(optimizer=optimizer_configs[0], loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_residual_analysis_plot(y_true, y_prob, model_name):\n",
    "    label_encoder = le()\n",
    "    y_true_encoded = label_encoder.fit_transform(y_true)\n",
    "    \n",
    "    residual_errors = []\n",
    "    class_residuals = {cls: [] for cls in ordered_classes}\n",
    "    \n",
    "    for true_label, probabilities in zip(y_true_encoded, y_prob):\n",
    "        residual_error = 1 - probabilities[true_label]\n",
    "        residual_errors.append(residual_error)\n",
    "        \n",
    "        true_class_name = label_encoder.classes_[true_label]\n",
    "        if true_class_name in class_residuals:\n",
    "            class_residuals[true_class_name].append(residual_error)\n",
    "    \n",
    "    residual_df = pd.DataFrame({'Residual Error': residual_errors})\n",
    "    \n",
    "    fig_configs = [\n",
    "        {'size': (6, 5), 'bins': 30, 'suffix': ''},\n",
    "        {'size': (8, 6), 'bins': 50, 'suffix': '_detailed'},\n",
    "        {'size': (10, 8), 'bins': 20, 'suffix': '_summary'}\n",
    "    ]\n",
    "    \n",
    "    for config in fig_configs:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=config['size'], dpi=600)\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        sns.histplot(residual_df['Residual Error'], bins=config['bins'], kde=True, color=\"#1E88E5\", \n",
    "                    stat=\"density\", ax=axes[0], alpha=0.7, edgecolor='black', linewidth=0.8)\n",
    "        axes[0].set_xlabel('Residual Error', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_title('Overall Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        colors = ['#e31a1c', '#ff7f00', '#1f78b4']\n",
    "        for i, (cls, errors) in enumerate(class_residuals.items()):\n",
    "            if errors and i < 3:\n",
    "                sns.histplot(errors, bins=config['bins']//2, kde=True, color=colors[i], \n",
    "                           alpha=0.6, ax=axes[i+1], stat=\"density\")\n",
    "                axes[i+1].set_xlabel('Residual Error', fontsize=12, fontweight='bold')\n",
    "                axes[i+1].set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "                axes[i+1].set_title(f'{cls} Class Residuals', fontsize=14, fontweight='bold')\n",
    "                axes[i+1].grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        for ax in axes:\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_linewidth(1.5)\n",
    "                spine.set_color('black')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        for ext in ['pdf', 'png', 'svg']:\n",
    "            filename = f\"residual_analysis_{model_name}{config['suffix']}.{ext}\"\n",
    "            plt.savefig(filename, format=ext, dpi=600, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def create_learning_curves(model, X, y, model_name):\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    \n",
    "    try:\n",
    "        train_sizes_abs, train_scores, val_scores = lc(model, X, y, train_sizes=train_sizes, cv=5, \n",
    "                                                      scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "        \n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6), dpi=600)\n",
    "        \n",
    "        ax.plot(train_sizes_abs, train_mean, 'o-', color='#1f77b4', label='Training Score', linewidth=2)\n",
    "        ax.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.2, color='#1f77b4')\n",
    "        \n",
    "        ax.plot(train_sizes_abs, val_mean, 'o-', color='#ff7f0e', label='Validation Score', linewidth=2)\n",
    "        ax.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.2, color='#ff7f0e')\n",
    "        \n",
    "        ax.set_xlabel('Training Set Size', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Accuracy Score', fontsize=14, fontweight='bold')\n",
    "        ax.set_title(f'Learning Curves - {model_name}', fontsize=16, fontweight='bold')\n",
    "        ax.legend(fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        for ext in ['pdf', 'png']:\n",
    "            plt.savefig(f\"learning_curves_{model_name}.{ext}\", format=ext, dpi=600, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Learning curve creation failed for {model_name}: {e}\")\n",
    "\n",
    "# Model training\n",
    "def execute_comprehensive_model_training():\n",
    "    training_results = {}\n",
    "    model_performances = {}\n",
    "    training_logs = []\n",
    "    \n",
    "    print(f\"Starting comprehensive training at {datetime.datetime.now()}\")\n",
    "    \n",
    "    for config_name, config in model_configurations.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {config_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            feature_set = feature_sets_config[config['features']]\n",
    "            target_column = config['target']\n",
    "            \n",
    "            available_features = [f for f in feature_set if f in df_work.columns]\n",
    "            if not available_features:\n",
    "                print(f\"No features available for {config_name}\")\n",
    "                continue\n",
    "            \n",
    "            X = df_work[available_features].copy()\n",
    "            y = df_work[target_column].copy()\n",
    "            \n",
    "            missing_data_summary = X.isnull().sum()\n",
    "            if missing_data_summary.sum() > 0:\n",
    "                print(f\"Missing data detected: {missing_data_summary.sum()} total missing values\")\n",
    "            \n",
    "            X = X.dropna()\n",
    "            y = y[X.index]\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(f\"No data remaining after cleaning for {config_name}\")\n",
    "                continue\n",
    "            \n",
    "            numerical_features = [f for f in available_features if f in df_work.select_dtypes(include=[np.number]).columns]\n",
    "            categorical_features = [f for f in available_features if f not in numerical_features]\n",
    "            \n",
    "            preprocessor = create_preprocessor(config['preprocessor'], numerical_features, categorical_features)\n",
    "            \n",
    "            X_preprocessed = preprocessor.fit_transform(X)\n",
    "            \n",
    "            if np.isnan(X_preprocessed).any():\n",
    "                X_preprocessed = np.nan_to_num(X_preprocessed, nan=0.0)\n",
    "            \n",
    "            X_resampled, y_resampled = apply_resampling(X_preprocessed, y, config['resampling'])\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = tts(X_resampled, y_resampled, test_size=0.2, \n",
    "                                                  random_state=42, stratify=y_resampled)\n",
    "            \n",
    "            training_start = time.time()\n",
    "            \n",
    "            if config['model'] == 'neural_network':\n",
    "                y_train_onehot = pd.get_dummies(y_train)\n",
    "                y_test_onehot = pd.get_dummies(y_test)\n",
    "                \n",
    "                model = create_neural_network_model(X_train.shape[1], y_train_onehot.shape[1], config['architecture'])\n",
    "                \n",
    "                callbacks_list = [\n",
    "                    es(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "                    rlp(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7),\n",
    "                    mcp(f'best_model_{config_name}.h5', save_best_only=True)\n",
    "                ]\n",
    "                \n",
    "                history = model.fit(X_train, y_train_onehot, \n",
    "                                  epochs=config['params']['epochs'][0],\n",
    "                                  batch_size=config['params']['batch_size'][0],\n",
    "                                  validation_split=0.2,\n",
    "                                  callbacks=callbacks_list,\n",
    "                                  verbose=1)\n",
    "                \n",
    "                y_pred_proba = model.predict(X_test)\n",
    "                y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "                y_test_classes = np.argmax(y_test_onehot.values, axis=1)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=600)\n",
    "                axes = axes.ravel()\n",
    "                \n",
    "                axes[0].plot(history.history['loss'], linewidth=2.5, color='#D81B60', label='Training Loss')\n",
    "                axes[0].plot(history.history['val_loss'], linewidth=2.5, color='#1E88E5', label='Validation Loss')\n",
    "                axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "                axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "                axes[0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                axes[1].plot(history.history['accuracy'], linewidth=2.5, color='#4CAF50', label='Training Accuracy')\n",
    "                axes[1].plot(history.history['val_accuracy'], linewidth=2.5, color='#FF9800', label='Validation Accuracy')\n",
    "                axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "                axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "                axes[1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                learning_rate_schedule = [float(callbacks_list[1].get_monitor_value(logs={'val_loss': 0.5})) for _ in range(len(history.history['loss']))]\n",
    "                axes[2].plot(learning_rate_schedule, linewidth=2, color='#9C27B0', label='Learning Rate')\n",
    "                axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "                axes[2].set_ylabel('Learning Rate', fontsize=12, fontweight='bold')\n",
    "                axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "                axes[2].legend()\n",
    "                axes[2].grid(True, alpha=0.3)\n",
    "                \n",
    "                epoch_times = np.diff([0] + list(range(len(history.history['loss']))))\n",
    "                axes[3].bar(range(len(epoch_times)), epoch_times, color='#607D8B', alpha=0.7)\n",
    "                axes[3].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "                axes[3].set_ylabel('Time (relative)', fontsize=12, fontweight='bold')\n",
    "                axes[3].set_title('Training Time per Epoch', fontsize=14, fontweight='bold')\n",
    "                axes[3].grid(True, alpha=0.3)\n",
    "                \n",
    "                for ax in axes:\n",
    "                    for spine in ax.spines.values():\n",
    "                        spine.set_linewidth(1.5)\n",
    "                        spine.set_color('black')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                for ext in ['pdf', 'png', 'svg']:\n",
    "                    plt.savefig(f\"training_history_{config_name}.{ext}\", format=ext, dpi=600, bbox_inches='tight')\n",
    "                \n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "                create_advanced_confusion_matrix(y_test_classes, y_pred, config_name, 'publication')\n",
    "                create_advanced_roc_curves(y_test_classes, y_pred_proba, model, config_name, 'publication')\n",
    "                create_residual_analysis_plot(y_test, y_pred_proba, config_name)\n",
    "                \n",
    "                best_model = model\n",
    "                y_test_for_eval = y_test_classes\n",
    "                y_pred_for_eval = y_pred\n",
    "                y_pred_proba_for_eval = y_pred_proba\n",
    "                \n",
    "            else:\n",
    "                if hasattr(config['model'], 'fit'):\n",
    "                    model = config['model']\n",
    "                    param_grid = config['params']\n",
    "                    \n",
    "                    grid_search = gscv(model, param_grid, cv=skf(n_splits=5, shuffle=True, random_state=42), \n",
    "                                     scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "                    \n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    \n",
    "                    print(f\"Best parameters for {config_name}: {grid_search.best_params_}\")\n",
    "                    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "                    \n",
    "                    cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "                    cv_results_df.to_excel(f'cv_results_{config_name}.xlsx', engine='xlsxwriter', index=False)\n",
    "                \n",
    "                y_pred_for_eval = best_model.predict(X_test)\n",
    "                y_pred_proba_for_eval = best_model.predict_proba(X_test) if hasattr(best_model, 'predict_proba') else None\n",
    "                y_test_for_eval = y_test\n",
    "                \n",
    "                create_advanced_confusion_matrix(y_test_for_eval, y_pred_for_eval, config_name, 'publication')\n",
    "                \n",
    "                if y_pred_proba_for_eval is not None:\n",
    "                    create_advanced_roc_curves(y_test_for_eval, y_pred_proba_for_eval, best_model, config_name, 'publication')\n",
    "                    create_residual_analysis_plot(y_test_for_eval, y_pred_proba_for_eval, config_name)\n",
    "                \n",
    "                create_learning_curves(best_model, X_resampled, y_resampled, config_name)\n",
    "            \n",
    "            training_end = time.time()\n",
    "            training_duration = training_end - training_start\n",
    "            \n",
    "            performance_metrics = {\n",
    "                'accuracy': acc(y_test_for_eval, y_pred_for_eval),\n",
    "                'precision_macro': ps(y_test_for_eval, y_pred_for_eval, average='macro'),\n",
    "                'recall_macro': rs_score(y_test_for_eval, y_pred_for_eval, average='macro'),\n",
    "                'f1_macro': f1(y_test_for_eval, y_pred_for_eval, average='macro'),\n",
    "                'f1_weighted': f1(y_test_for_eval, y_pred_for_eval, average='weighted'),\n",
    "                'training_time': training_duration\n",
    "            }\n",
    "            \n",
    "            if y_pred_proba_for_eval is not None:\n",
    "                try:\n",
    "                    performance_metrics['log_loss'] = ll(pd.get_dummies(y_test_for_eval), y_pred_proba_for_eval)\n",
    "                    performance_metrics['roc_auc_macro'] = ras(pd.get_dummies(y_test_for_eval), y_pred_proba_for_eval, average='macro', multi_class='ovr')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"\\n{config_name.upper()} Performance Metrics:\")\n",
    "            for metric, value in performance_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\" if isinstance(value, float) else f\"{metric}: {value}\")\n",
    "            \n",
    "            classification_rep = cr(y_test_for_eval, y_pred_for_eval, output_dict=True)\n",
    "            classification_df = pd.DataFrame(classification_rep).transpose()\n",
    "            classification_df.to_excel(f'classification_report_{config_name}.xlsx', engine='xlsxwriter')\n",
    "            \n",
    "            joblib.dump({\n",
    "                'model': best_model,\n",
    "                'preprocessor': preprocessor,\n",
    "                'feature_names': available_features,\n",
    "                'performance_metrics': performance_metrics,\n",
    "                'config': config\n",
    "            }, f'complete_model_{config_name}.pkl')\n",
    "            \n",
    "            training_results[config_name] = {\n",
    "                'model': best_model,\n",
    "                'preprocessor': preprocessor,\n",
    "                'performance': performance_metrics,\n",
    "                'predictions': y_pred_for_eval,\n",
    "                'probabilities': y_pred_proba_for_eval,\n",
    "                'test_labels': y_test_for_eval\n",
    "            }\n",
    "            \n",
    "            model_performances[config_name] = performance_metrics\n",
    "            \n",
    "            training_log = {\n",
    "                'model_name': config_name,\n",
    "                'timestamp': datetime.datetime.now().isoformat(),\n",
    "                'duration': training_duration,\n",
    "                'accuracy': performance_metrics['accuracy'],\n",
    "                'f1_weighted': performance_metrics['f1_weighted'],\n",
    "                'status': 'success'\n",
    "            }\n",
    "            training_logs.append(training_log)\n",
    "            \n",
    "            print(f\"\\n{config_name} training completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_log = {\n",
    "                'model_name': config_name,\n",
    "                'timestamp': datetime.datetime.now().isoformat(),\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            training_logs.append(error_log)\n",
    "            print(f\"Error training {config_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    training_summary_df = pd.DataFrame(training_logs)\n",
    "    training_summary_df.to_excel('training_summary.xlsx', engine='xlsxwriter', index=False)\n",
    "    \n",
    "    performance_comparison_df = pd.DataFrame(model_performances).transpose()\n",
    "    performance_comparison_df.to_excel('model_performance_comparison.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "    return training_results, model_performances, training_logs\n",
    "\n",
    "def execute_ensemble_model_training():\n",
    "    ensemble_results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING ENSEMBLE MODELS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for ensemble_name, ensemble_config in ensemble_configurations.items():\n",
    "        print(f\"\\nTraining {ensemble_name}...\")\n",
    "        \n",
    "        try:\n",
    "            feature_set = feature_sets_config[ensemble_config['features']]\n",
    "            target_column = ensemble_config['target']\n",
    "            \n",
    "            available_features = [f for f in feature_set if f in df_work.columns]\n",
    "            X = df_work[available_features].dropna()\n",
    "            y = df_work[target_column][X.index]\n",
    "            \n",
    "            numerical_features = [f for f in available_features if f in df_work.select_dtypes(include=[np.number]).columns]\n",
    "            categorical_features = [f for f in available_features if f not in numerical_features]\n",
    "            \n",
    "            preprocessor = create_preprocessor(ensemble_config['preprocessor'], numerical_features, categorical_features)\n",
    "            X_preprocessed = preprocessor.fit_transform(X)\n",
    "            \n",
    "            X_resampled, y_resampled = apply_resampling(X_preprocessed, y, ensemble_config['resampling'])\n",
    "            X_train, X_test, y_train, y_test = tts(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "            \n",
    "            if ensemble_config['type'] == 'voting':\n",
    "                base_estimators = []\n",
    "                for base_model_name in ensemble_config['base_models']:\n",
    "                    if base_model_name in model_configurations:\n",
    "                        base_config = model_configurations[base_model_name]\n",
    "                        model_instance = base_config['model']\n",
    "                        base_estimators.append((base_model_name, model_instance))\n",
    "                \n",
    "                ensemble_model = vc_cls(estimators=base_estimators, voting=ensemble_config['voting'])\n",
    "                \n",
    "            elif ensemble_config['type'] == 'stacking':\n",
    "                ensemble_model = sc(\n",
    "                    estimators=[(f'base_{i}', model) for i, model in enumerate(ensemble_config['base_models'])],\n",
    "                    final_estimator=ensemble_config['meta_model'],\n",
    "                    cv=skf(n_splits=5, shuffle=True, random_state=42)\n",
    "                )\n",
    "            \n",
    "            ensemble_model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = ensemble_model.predict(X_test)\n",
    "            y_pred_proba = ensemble_model.predict_proba(X_test) if hasattr(ensemble_model, 'predict_proba') else None\n",
    "            \n",
    "            performance_metrics = {\n",
    "                'accuracy': acc(y_test, y_pred),\n",
    "                'f1_weighted': f1(y_test, y_pred, average='weighted'),\n",
    "                'f1_macro': f1(y_test, y_pred, average='macro')\n",
    "            }\n",
    "            \n",
    "            print(f\"{ensemble_name} Performance:\")\n",
    "            for metric, value in performance_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            \n",
    "            create_advanced_confusion_matrix(y_test, y_pred, ensemble_name, 'publication')\n",
    "            if y_pred_proba is not None:\n",
    "                create_advanced_roc_curves(y_test, y_pred_proba, ensemble_model, ensemble_name, 'publication')\n",
    "            \n",
    "            joblib.dump({\n",
    "                'model': ensemble_model,\n",
    "                'preprocessor': preprocessor,\n",
    "                'performance': performance_metrics\n",
    "            }, f'ensemble_model_{ensemble_name}.pkl')\n",
    "            \n",
    "            ensemble_results[ensemble_name] = {\n",
    "                'model': ensemble_model,\n",
    "                'performance': performance_metrics\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ensemble {ensemble_name} training failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return ensemble_results\n",
    "\n",
    "def generate_comprehensive_analysis_report(training_results, ensemble_results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING COMPREHENSIVE ANALYSIS REPORT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    all_results = {**training_results, **ensemble_results}\n",
    "    \n",
    "    performance_summary = []\n",
    "    for model_name, result in all_results.items():\n",
    "        if 'performance' in result:\n",
    "            perf = result['performance']\n",
    "            summary_row = {\n",
    "                'Model': model_name,\n",
    "                'Accuracy': perf.get('accuracy', 0),\n",
    "                'F1_Weighted': perf.get('f1_weighted', 0),\n",
    "                'F1_Macro': perf.get('f1_macro', 0),\n",
    "                'Training_Time': perf.get('training_time', 0)\n",
    "            }\n",
    "            performance_summary.append(summary_row)\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_summary)\n",
    "    performance_df = performance_df.sort_values('F1_Weighted', ascending=False)\n",
    "    \n",
    "    performance_df.to_excel('final_performance_summary.xlsx', engine='xlsxwriter', index=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12), dpi=600)\n",
    "    \n",
    "    axes[0, 0].bar(performance_df['Model'], performance_df['Accuracy'], color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontweight='bold')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    axes[0, 1].bar(performance_df['Model'], performance_df['F1_Weighted'], color='lightgreen', alpha=0.7)\n",
    "    axes[0, 1].set_title('F1-Weighted Score Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('F1-Weighted', fontweight='bold')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    axes[1, 0].bar(performance_df['Model'], performance_df['F1_Macro'], color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_title('F1-Macro Score Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('F1-Macro', fontweight='bold')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    training_times = performance_df[performance_df['Training_Time'] > 0]\n",
    "    if not training_times.empty:\n",
    "        axes[1, 1].bar(training_times['Model'], training_times['Training_Time'], color='red', alpha=0.7)\n",
    "        axes[1, 1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Time (seconds)', fontweight='bold')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for ext in ['pdf', 'png', 'svg']:\n",
    "        plt.savefig(f'model_comparison_summary.{ext}', format=ext, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    best_model_name = performance_df.iloc[0]['Model']\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"Best F1-Weighted score: {performance_df.iloc[0]['F1_Weighted']:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    final_report = {\n",
    "        'experiment_id': dt_hash,\n",
    "        'total_duration': total_duration,\n",
    "        'models_trained': len(all_results),\n",
    "        'best_model': best_model_name,\n",
    "        'best_score': performance_df.iloc[0]['F1_Weighted'],\n",
    "        'timestamp': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open('experiment_summary.json', 'w') as f:\n",
    "        json.dump(final_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nExperiment completed in {total_duration:.2f} seconds\")\n",
    "    print(f\"Total models trained: {len(all_results)}\")\n",
    "    print(\"\\nFiles generated:\")\n",
    "    \n",
    "    file_list = [\n",
    "        'final_performance_summary.xlsx',\n",
    "        'model_comparison_summary.pdf',\n",
    "        'experiment_summary.json'\n",
    "    ]\n",
    "    \n",
    "    for model_name in all_results.keys():\n",
    "        file_list.extend([\n",
    "            f'confusion_matrix_{model_name}_publication.pdf',\n",
    "            f'roc_curve_{model_name}_publication.pdf',\n",
    "            f'classification_report_{model_name}.xlsx',\n",
    "            f'complete_model_{model_name}.pkl'\n",
    "        ])\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if os.path.exists(file_name):\n",
    "            print(f\" {file_name}\")\n",
    "    \n",
    "    return final_report\n",
    "\n",
    "# System execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE MACHINE LEARNING PIPELINE EXECUTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Data hash: {dt_hash}\")\n",
    "    print(f\"Start time: {datetime.datetime.now()}\")\n",
    "    \n",
    "    try:\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"\\nPhase 1: Individual Model Training\")\n",
    "        training_results, model_performances, training_logs = execute_comprehensive_model_training()\n",
    "        \n",
    "        print(\"\\nPhase 2: Ensemble Model Training\") \n",
    "        ensemble_results = execute_ensemble_model_training()\n",
    "        \n",
    "        print(\"\\nPhase 3: Comprehensive Analysis\")\n",
    "        final_report = generate_comprehensive_analysis_report(training_results, ensemble_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        successful_models = len([log for log in training_logs if log.get('status') == 'success'])\n",
    "        failed_models = len([log for log in training_logs if log.get('status') == 'failed'])\n",
    "        \n",
    "        print(f\"Successful models: {successful_models}\")\n",
    "        print(f\"Failed models: {failed_models}\")\n",
    "        print(f\"Total execution time: {final_report['total_duration']:.2f} seconds\")\n",
    "        print(f\"Best model: {final_report['best_model']}\")\n",
    "        print(f\"Best score: {final_report['best_score']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nDiagnostic Information:\")\n",
    "        print(f\"Python version: {sys.version}\")\n",
    "        print(f\"Working directory: {os.getcwd()}\")\n",
    "        print(f\"Data file exists: {os.path.exists(fp_main)}\")\n",
    "        print(f\"Available memory: {gc.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ee2e2",
   "metadata": {},
   "source": [
    "### Updated ANN_SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d517d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "[os.environ.update({k: '1'}) for k in ['OMP_NUM_THREADS','OPENBLAS_NUM_THREADS','MKL_NUM_THREADS','VECLIB_MAXIMUM_THREADS','NUMEXPR_NUM_THREADS','TF_NUM_INTEROP_THREADS','TF_NUM_INTRAOP_THREADS']]\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers as lyr, models as mdl, regularizers as reg, callbacks as cb\n",
    "from sklearn.model_selection import train_test_split as tts, GridSearchCV as gscv, StratifiedKFold as skf\n",
    "from sklearn.preprocessing import StandardScaler as ss, OneHotEncoder as ohe, LabelEncoder as le, RobustScaler as rs, PowerTransformer as pt\n",
    "from sklearn.compose import ColumnTransformer as ct\n",
    "from sklearn.pipeline import Pipeline as pl\n",
    "from sklearn.svm import SVC as svc\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc, GradientBoostingClassifier as gbc\n",
    "from sklearn.metrics import classification_report as cr, roc_curve as rc, auc as auc_fn, confusion_matrix as cm, f1_score as f1, accuracy_score as acc_s, balanced_accuracy_score as ba, cohen_kappa_score as ck, matthews_corrcoef as mc\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from imblearn.over_sampling import SMOTE as sm, ADASYN as ad, BorderlineSMOTE as bsm\n",
    "from imblearn.under_sampling import EditedNearestNeighbours as enn, TomekLinks as tl\n",
    "from imblearn.combine import SMOTETomek as smt\n",
    "from imblearn.pipeline import Pipeline as ipl\n",
    "from sklearn.base import BaseEstimator as be, ClassifierMixin as cmx\n",
    "from sklearn.feature_selection import SelectKBest as skb, f_classif as fc, RFE as rfe\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap as lscm\n",
    "import seaborn as sns\n",
    "import json, joblib, xlsxwriter, warnings, sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pf = \"plots\"\n",
    "if not os.path.exists(pf): os.makedirs(pf); print(f\"Created plots folder: {pf}\")\n",
    "sv = sklearn.__version__; print(f\"Using scikit-learn version: {sv}\")\n",
    "\n",
    "def get_ohe(**kwargs):\n",
    "    vm, vn = map(int, sv.split('.')[:2])\n",
    "    return ohe(sparse_output=False, **kwargs) if vm > 1 or (vm == 1 and vn >= 2) else ohe(sparse=False, **kwargs)\n",
    "\n",
    "def safe_smote_resample(X, y, random_state=42):\n",
    "    try:\n",
    "        smote = sm(random_state=random_state, k_neighbors=min(5, len(np.unique(y))-1), n_jobs=1)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        print(\"Successfully applied SMOTE resampling\"); return X_resampled, y_resampled\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE failed: {e}\")\n",
    "        try:\n",
    "            from imblearn.over_sampling import RandomOverSampler as ros\n",
    "            ros_obj = ros(random_state=random_state)\n",
    "            X_resampled, y_resampled = ros_obj.fit_resample(X, y)\n",
    "            print(\"Applied RandomOverSampler as fallback\"); return X_resampled, y_resampled\n",
    "        except Exception as e2:\n",
    "            print(f\"RandomOverSampler also failed: {e2}\"); print(\"Using original data without resampling\")\n",
    "            return X, y\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial', 'font.size': 11, 'font.weight': 'bold', 'axes.linewidth': 1.2,\n",
    "    'axes.labelsize': 14, 'axes.labelweight': 'bold', 'axes.titlesize': 16, 'axes.titleweight': 'bold',\n",
    "    'xtick.major.width': 1.2, 'ytick.major.width': 1.2, 'xtick.major.size': 5, 'ytick.major.size': 5,\n",
    "    'xtick.labelsize': 12, 'ytick.labelsize': 12, 'legend.frameon': False, 'legend.fontsize': 11,\n",
    "    'figure.dpi': 600, 'savefig.dpi': 600, 'savefig.bbox': 'tight', 'savefig.pad_inches': 0.1,\n",
    "})\n",
    "\n",
    "def load_and_preprocess_data(fp):\n",
    "    df = pd.read_excel(fp)\n",
    "    tgt = 'Failure_Score'\n",
    "    fts = ['Flowline_Age', 'MAXOPPRESSURE', 'oil_spilled', 'PW_spilled', 'cond_spilled', 'other_spilled', \n",
    "           'MAXOD', 'LAT', 'LONG', 'WeatherConditions_Freq_Encoded', 'Root_Cause_Freq_Encoded', \n",
    "           'Spill_Type_Target_Encoded', 'Typeoffluidtrans_Encoded', 'Pipematerial_Freq_Encoded', \n",
    "           'Basin_Freq_Encoded', 'Flowlinetype_Freq_Encoded']\n",
    "    \n",
    "    print(\"Applying frequency encoding...\")\n",
    "    rcf = df['Root Cause Type'].value_counts().to_dict(); df['Root_Cause_Freq_Encoded'] = df['Root Cause Type'].map(rcf)\n",
    "    ttf = df['TYPEOFFLUIDTRANS'].value_counts().to_dict(); df['Typeoffluidtrans_Encoded'] = df['TYPEOFFLUIDTRANS'].map(ttf)\n",
    "    pmf = df['PIPEMATERIAL'].value_counts().to_dict(); df['Pipematerial_Freq_Encoded'] = df['PIPEMATERIAL'].map(pmf)\n",
    "    bf = df['Basin'].value_counts().to_dict(); df['Basin_Freq_Encoded'] = df['Basin'].map(bf)\n",
    "    ftf = df['FLOWLINETYPE'].value_counts().to_dict(); df['Flowlinetype_Freq_Encoded'] = df['FLOWLINETYPE'].map(ftf)\n",
    "    wcf = df['Weather Conditions'].value_counts().to_dict(); df['WeatherConditions_Freq_Encoded'] = df['Weather Conditions'].map(wcf)\n",
    "    \n",
    "    print(\"Applying target encoding...\")\n",
    "    sttm = df.groupby('Spill Type_x')[tgt].mean(numeric_only=True).to_dict()\n",
    "    df['Spill_Type_Target_Encoded'] = df['Spill Type_x'].map(sttm)\n",
    "    \n",
    "    print(\"Creating failure categories...\")\n",
    "    lt, ht = df[tgt].quantile(0.35), df[tgt].quantile(0.70)\n",
    "    cs = lambda s: 'Low' if s < lt else ('Medium' if s < ht else 'High')\n",
    "    df['Failure_Category'] = df[tgt].apply(cs)\n",
    "    df = df.dropna(subset=[tgt] + fts + ['Failure_Category'])\n",
    "    \n",
    "    print(f\"Data preprocessing completed. Final dataset shape: {df.shape}\")\n",
    "    print(f\"Failure distribution:\\n{df['Failure_Category'].value_counts()}\")\n",
    "    \n",
    "    return df, fts, tgt\n",
    "\n",
    "class ANNSVMEnsemble(be, cmx):\n",
    "    def __init__(self, em='weighted_voting', aw=0.6, sw=0.4, vb=True):\n",
    "        self.em, self.aw, self.sw, self.vb = em, aw, sw, vb\n",
    "        self.am, self.sm, self.ml, self.pp, self.le = None, None, None, None, None\n",
    "        self.cn = ['High', 'Medium', 'Low']\n",
    "        \n",
    "    def bam(self, id, od):\n",
    "        m = mdl.Sequential([lyr.Dense(128, input_dim=id, activation='relu'), lyr.Dropout(0.3),\n",
    "                           lyr.Dense(64, activation='relu'), lyr.Dropout(0.3),\n",
    "                           lyr.Dense(32, activation='relu'), lyr.Dropout(0.3),\n",
    "                           lyr.Dense(od, activation='softmax')])\n",
    "        m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']); return m\n",
    "    \n",
    "    def bsp(self):\n",
    "        return svc(probability=True, kernel='rbf', random_state=42, class_weight='balanced')\n",
    "    \n",
    "    def fit(self, Xt, yt, vs=0.2):\n",
    "        if self.vb: print(\"Training Original ANN-SVM Ensemble...\")\n",
    "        \n",
    "        self.le = le(); ye = self.le.fit_transform(yt)\n",
    "        \n",
    "        cf = ['WeatherConditions_Freq_Encoded', 'Root_Cause_Freq_Encoded', \n",
    "              'Spill_Type_Target_Encoded', 'Typeoffluidtrans_Encoded', \n",
    "              'Pipematerial_Freq_Encoded', 'Basin_Freq_Encoded', 'Flowlinetype_Freq_Encoded']\n",
    "        nf = ['Flowline_Age', 'MAXOPPRESSURE', 'oil_spilled', 'PW_spilled', 'cond_spilled', \n",
    "              'other_spilled', 'MAXOD', 'LAT', 'LONG']\n",
    "        \n",
    "        self.pp = ct(transformers=[('num', ss(), nf), ('cat', get_ohe(handle_unknown='ignore'), cf)])\n",
    "        Xp = self.pp.fit_transform(Xt)\n",
    "        \n",
    "        Xab, yab = safe_smote_resample(Xp, ye)\n",
    "        yao = tf.keras.utils.to_categorical(yab, num_classes=len(self.cn))\n",
    "        Xat, Xav, yat, yav = tts(Xab, yao, test_size=vs, random_state=42, stratify=yab)\n",
    "        \n",
    "        id, od = Xat.shape[1], len(self.cn)\n",
    "        self.am = self.bam(id, od)\n",
    "        es = cb.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.am.fit(Xat, yat, epochs=100, batch_size=32, validation_data=(Xav, yav),\n",
    "                   callbacks=[es], verbose=1 if self.vb else 0)\n",
    "        \n",
    "        Xsb, ysb = safe_smote_resample(Xp, ye)\n",
    "        sbm = self.bsp()\n",
    "        pg = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['rbf']}\n",
    "        sg = gscv(sbm, pg, cv=5, scoring='accuracy', n_jobs=1, verbose=0)\n",
    "        sg.fit(Xsb, ysb); self.sm = sg.best_estimator_\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        Xp = self.pp.transform(X)\n",
    "        ap = self.am.predict(Xp, verbose=0); sp = self.sm.predict_proba(Xp)\n",
    "        sc = self.sm.classes_; aco = [0, 1, 2]\n",
    "        \n",
    "        if not np.array_equal(sc, aco):\n",
    "            ri = [np.where(sc == i)[0][0] for i in aco]; sp = sp[:, ri]\n",
    "        \n",
    "        ep = self.aw * ap + self.sw * sp; return ep\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pb = self.predict_proba(X); return np.argmax(pb, axis=1)\n",
    "    \n",
    "    def save_models(self, bp):\n",
    "        self.am.save(f\"{bp}_ann_model.h5\"); joblib.dump(self.sm, f\"{bp}_svm_model.pkl\")\n",
    "        joblib.dump(self.pp, f\"{bp}_preprocessor.pkl\"); joblib.dump(self.le, f\"{bp}_label_encoder.pkl\")\n",
    "        print(f\"Ensemble models saved with base path: {bp}\")\n",
    "\n",
    "# Processing workflow\n",
    "class AdvancedANNSVMEnsemble(be, cmx):\n",
    "    def __init__(self, em='adaptive_weighted', uaf=True, ufl=True, vb=True):\n",
    "        self.em, self.uaf, self.ufl, self.vb = em, uaf, ufl, vb\n",
    "        self.am, self.sm, self.rm, self.ml, self.pp, self.le, self.fs = [None]*7\n",
    "        self.cn, self.mw = ['High', 'Medium', 'Low'], {'ann': 0.4, 'svm': 0.4, 'rf': 0.2}\n",
    "\n",
    "    def fl(self, a=0.25, g=2.0):\n",
    "        def flf(yt, yp):\n",
    "            yp = tf.clip_by_value(yp, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            ce = -yt * tf.math.log(yp); w = a * yt * tf.math.pow((1 - yp), g)\n",
    "            fl = w * ce; return tf.reduce_mean(tf.reduce_sum(fl, axis=1))\n",
    "        return flf\n",
    "\n",
    "    def baam(self, id, od):\n",
    "        m = mdl.Sequential([\n",
    "            lyr.Dense(256, input_dim=id), lyr.BatchNormalization(), lyr.Activation('relu'), lyr.Dropout(0.4),\n",
    "            lyr.Dense(128, kernel_regularizer=reg.l2(0.001)), lyr.BatchNormalization(), lyr.Activation('relu'), lyr.Dropout(0.3),\n",
    "            lyr.Dense(64, kernel_regularizer=reg.l2(0.001)), lyr.BatchNormalization(), lyr.Activation('relu'), lyr.Dropout(0.2),\n",
    "            lyr.Dense(od, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        lf = self.fl(alpha=0.25, gamma=2.0) if self.ufl else 'categorical_crossentropy'\n",
    "        m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=lf, metrics=['accuracy']); return m\n",
    "\n",
    "    def caf(self, X):\n",
    "        if not self.uaf: return X\n",
    "        Xe = X.copy()\n",
    "        \n",
    "        if 'Flowline_Age' in X.columns and 'MAXOPPRESSURE' in X.columns:\n",
    "            Xe['Age_Pressure_Interaction'] = X['Flowline_Age'] * X['MAXOPPRESSURE']\n",
    "            r = X['Flowline_Age'] / (X['MAXOPPRESSURE'] + 1e-8)\n",
    "            Xe['Age_Pressure_Ratio'] = np.where(np.isfinite(r), r, 0)\n",
    "        \n",
    "        if all(col in X.columns for col in ['oil_spilled', 'PW_spilled', 'cond_spilled', 'other_spilled']):\n",
    "            os = X['oil_spilled'].fillna(0); ps = X['PW_spilled'].fillna(0)\n",
    "            cs = X['cond_spilled'].fillna(0); ots = X['other_spilled'].fillna(0)\n",
    "            Xe['Total_Spill_Volume'] = os + ps + cs + ots\n",
    "            Xe['Spill_Diversity_Index'] = ((os > 0).astype(int) + (ps > 0).astype(int) + \n",
    "                                          (cs > 0).astype(int) + (ots > 0).astype(int))\n",
    "        \n",
    "        if 'Flowline_Age' in X.columns:\n",
    "            as_ = X['Flowline_Age'].fillna(0)\n",
    "            Xe['Age_Squared'] = as_ ** 2; Xe['Age_Log'] = np.log1p(np.maximum(as_, 0))\n",
    "        \n",
    "        Xe = Xe.replace([np.inf, -np.inf], 0); return Xe\n",
    "\n",
    "    def fit(self, Xt, yt, vs=0.2):\n",
    "        if self.vb: print(\"Training Advanced ANN-SVM Ensemble...\")\n",
    "        \n",
    "        Xte = self.caf(Xt)\n",
    "        if Xte.isnull().any().any():\n",
    "            print(\"Warning: NaN values detected after feature engineering. Filling with median/mode...\")\n",
    "            nc = Xte.select_dtypes(include=[np.number]).columns\n",
    "            Xte[nc] = Xte[nc].fillna(Xte[nc].median())\n",
    "            cc = Xte.select_dtypes(exclude=[np.number]).columns\n",
    "            for col in cc:\n",
    "                Xte[col] = Xte[col].fillna(Xte[col].mode().iloc[0] if not Xte[col].mode().empty else 0)\n",
    "        \n",
    "        self.le = le(); ye = self.le.fit_transform(yt)\n",
    "        cf = [col for col in Xte.columns if 'Encoded' in col or 'Bin' in col]\n",
    "        nf = [col for col in Xte.columns if col not in cf]\n",
    "        \n",
    "        from sklearn.impute import SimpleImputer as si\n",
    "        self.pp = ct(transformers=[\n",
    "            ('num', pl([('imputer', si(strategy='median')), ('scaler', rs())]), nf),\n",
    "            ('cat', pl([('imputer', si(strategy='most_frequent')), ('encoder', get_ohe(handle_unknown='ignore'))]), cf)\n",
    "        ])\n",
    "        \n",
    "        Xp = self.pp.fit_transform(Xte)\n",
    "        if np.isnan(Xp).any(): print(\"Warning: NaN values still present after preprocessing. Replacing with zeros...\"); Xp = np.nan_to_num(Xp, nan=0.0)\n",
    "        \n",
    "        self.fs = skb(score_func=fc, k='all'); Xs = self.fs.fit_transform(Xp, ye)\n",
    "        \n",
    "        try:\n",
    "            rs_obj = smt(smote=sm(random_state=42, k_neighbors=min(5, len(np.unique(ye))-1), n_jobs=1),\n",
    "                        tomek=tl(n_jobs=1), random_state=42)\n",
    "            Xr, yr = rs_obj.fit_resample(Xs, ye); print(\"Applied SMOTETomek resampling\")\n",
    "        except Exception as e:\n",
    "            print(f\"SMOTETomek failed: {e}\"); Xr, yr = safe_smote_resample(Xs, ye)\n",
    "        \n",
    "        if self.vb: print(\"Training Advanced ANN model...\")\n",
    "        yao = tf.keras.utils.to_categorical(yr, num_classes=len(self.cn))\n",
    "        Xat, Xav, yat, yav = tts(Xr, yao, test_size=vs, random_state=42, stratify=yr)\n",
    "        \n",
    "        self.am = self.baam(Xat.shape[1], len(self.cn))\n",
    "        cbs = [cb.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "               cb.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)]\n",
    "        self.am.fit(Xat, yat, epochs=150, batch_size=32, validation_data=(Xav, yav),\n",
    "                   callbacks=cbs, verbose=1 if self.vb else 0)\n",
    "        \n",
    "        if self.vb: print(\"Training Advanced SVM model...\")\n",
    "        sm_obj = svc(probability=True, random_state=42, class_weight='balanced')\n",
    "        pg = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1], 'kernel': ['rbf', 'poly']}\n",
    "        sg = gscv(sm_obj, pg, cv=5, scoring='f1_weighted', n_jobs=1, verbose=0)\n",
    "        sg.fit(Xr, yr); self.sm = sg.best_estimator_\n",
    "        \n",
    "        if self.vb: print(\"Training Random Forest for diversity...\")\n",
    "        rm_obj = rfc(n_estimators=200, max_depth=10, min_samples_split=5,\n",
    "                    min_samples_leaf=2, class_weight='balanced', random_state=42)\n",
    "        self.rm = rm_obj.fit(Xr, yr)\n",
    "        \n",
    "        if self.em in ['adaptive_weighted', 'hierarchical']:\n",
    "            if self.vb: print(\"Training meta-learner...\")\n",
    "            ap = self.am.predict(Xr, verbose=0); sp = self.sm.predict_proba(Xr); rp = self.rm.predict_proba(Xr)\n",
    "            mf = np.hstack([ap, sp, rp])\n",
    "            ac, sc, rc = np.max(ap, axis=1), np.max(sp, axis=1), np.max(rp, axis=1)\n",
    "            cf_obj = np.column_stack([ac, sc, rc]); mf = np.hstack([mf, cf_obj])\n",
    "            self.ml = gbc(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "            self.ml.fit(mf, yr)\n",
    "        \n",
    "        if self.vb: print(\"Advanced ensemble training completed!\"); return None\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xe = self.caf(X)\n",
    "        if Xe.isnull().any().any():\n",
    "            nc = Xe.select_dtypes(include=[np.number]).columns; Xe[nc] = Xe[nc].fillna(Xe[nc].median())\n",
    "            cc = Xe.select_dtypes(exclude=[np.number]).columns\n",
    "            for col in cc:\n",
    "                Xe[col] = Xe[col].fillna(Xe[col].mode().iloc[0] if not Xe[col].mode().empty else 0)\n",
    "        \n",
    "        Xp = self.pp.transform(Xe)\n",
    "        if np.isnan(Xp).any(): Xp = np.nan_to_num(Xp, nan=0.0)\n",
    "        Xs = self.fs.transform(Xp)\n",
    "        \n",
    "        ap = self.am.predict(Xs, verbose=0); sp = self.sm.predict_proba(Xs); rp = self.rm.predict_proba(Xs)\n",
    "        \n",
    "        sc, rc, to = self.sm.classes_, self.rm.classes_, [0, 1, 2]\n",
    "        if not np.array_equal(sc, to):\n",
    "            ri = [np.where(sc == i)[0][0] for i in to]; sp = sp[:, ri]\n",
    "        if not np.array_equal(rc, to):\n",
    "            ri = [np.where(rc == i)[0][0] for i in to]; rp = rp[:, ri]\n",
    "        \n",
    "        if self.em == 'adaptive_weighted':\n",
    "            ac, sc_conf, rc_conf = np.max(ap, axis=1), np.max(sp, axis=1), np.max(rp, axis=1)\n",
    "            tc = ac + sc_conf + rc_conf\n",
    "            aw, sw, rw = ac / tc, sc_conf / tc, rc_conf / tc\n",
    "            ep = (aw[:, np.newaxis] * ap + sw[:, np.newaxis] * sp + rw[:, np.newaxis] * rp)\n",
    "            \n",
    "        elif self.em == 'hierarchical' and self.ml is not None:\n",
    "            mf = np.hstack([ap, sp, rp])\n",
    "            ac, sc_conf, rc_conf = np.max(ap, axis=1), np.max(sp, axis=1), np.max(rp, axis=1)\n",
    "            cf_obj = np.column_stack([ac, sc_conf, rc_conf]); mf = np.hstack([mf, cf_obj])\n",
    "            epe = self.ml.predict_proba(mf)\n",
    "            ep = np.zeros((len(X), len(self.cn)))\n",
    "            for i, ci in enumerate(self.ml.classes_): ep[:, ci] = epe[:, i]\n",
    "        \n",
    "        else: ep = (self.mw['ann'] * ap + self.mw['svm'] * sp + self.mw['rf'] * rp)\n",
    "        \n",
    "        return ep\n",
    "\n",
    "    def predict(self, X):\n",
    "        pb = self.predict_proba(X); return np.argmax(pb, axis=1)\n",
    "    \n",
    "    def save_models(self, bp):\n",
    "        self.am.save(f\"{bp}_ann_model.h5\"); joblib.dump(self.sm, f\"{bp}_svm_model.pkl\")\n",
    "        joblib.dump(self.rm, f\"{bp}_rf_model.pkl\"); joblib.dump(self.pp, f\"{bp}_preprocessor.pkl\")\n",
    "        joblib.dump(self.le, f\"{bp}_label_encoder.pkl\"); joblib.dump(self.fs, f\"{bp}_feature_selector.pkl\")\n",
    "        if self.ml: joblib.dump(self.ml, f\"{bp}_meta_learner.pkl\")\n",
    "        print(f\"Advanced ensemble models saved with base path: {bp}\")\n",
    "\n",
    "def advanced_model_evaluation(ens, Xt, yt, cn=['High', 'Medium', 'Low']):\n",
    "    ypp = ens.predict_proba(Xt); ypc = np.argmax(ypp, axis=1); yte = ens.le.transform(yt)\n",
    "    \n",
    "    acc = acc_s(yte, ypc); ba_val = ba(yte, ypc)\n",
    "    f1w = f1(yte, ypc, average='weighted'); f1m = f1(yte, ypc, average='macro')\n",
    "    kp = ck(yte, ypc); mcc_val = mc(yte, ypc)\n",
    "    \n",
    "    print(f\"\\nAdvanced Evaluation Metrics:\"); print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {ba_val:.4f}\"); print(f\"F1-Score (Weighted): {f1w:.4f}\")\n",
    "    print(f\"F1-Score (Macro): {f1m:.4f}\"); print(f\"Cohen's Kappa: {kp:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc_val:.4f}\")\n",
    "    print(f\"\\nDetailed Classification Report:\"); print(cr(yte, ypc, target_names=cn))\n",
    "    \n",
    "    return {'accuracy': acc, 'balanced_accuracy': ba_val, 'f1_weighted': f1w, 'f1_macro': f1m, 'kappa': kp, 'mcc': mcc_val}\n",
    "\n",
    "def run_enhanced_ensemble_comparison():\n",
    "    print(\"=\"*80); print(\"ENHANCED ANN-SVM ENSEMBLE WITH ADVANCED TECHNIQUES\"); print(\"=\"*80)\n",
    "    \n",
    "    # Data loading pipeline\n",
    "    ef = \"C:/Users/anisi/OneDrive - Colorado School of Mines/Documents/PhD/PhD Research/ECMC/Spills/Spills_Data.xlsx\"\n",
    "    print(\"Loading and preprocessing ECMC data...\"); dt, fts, tgt = load_and_preprocess_data(ef)\n",
    "    \n",
    "    X, y = dt[fts], dt['Failure_Category']\n",
    "    print(f\"Dataset shape: {X.shape}\"); print(f\"Class distribution:\\n{y.value_counts()}\")\n",
    "    \n",
    "    Xt, Xte, yt, yte = tts(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    rc = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"TRAINING ORIGINAL ENSEMBLE\"); print(\"=\"*60)\n",
    "    oe = ANNSVMEnsemble(em='weighted_voting', vb=True); oe.fit(Xt, yt)\n",
    "    ypo = oe.predict(Xte); ytee = oe.le.transform(yte); oa = acc_s(ytee, ypo)\n",
    "    rc['original'] = {'accuracy': oa, 'model': oe, 'predictions': ypo}\n",
    "    print(f\"Original Ensemble Accuracy: {oa:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"TRAINING ENHANCED ENSEMBLE WITH FOCAL LOSS\"); print(\"=\"*60)\n",
    "    ef_obj = AdvancedANNSVMEnsemble(em='adaptive_weighted', uaf=True, ufl=True, vb=True)\n",
    "    ef_obj.fit(Xt, yt); emf = advanced_model_evaluation(ef_obj, Xte, yte)\n",
    "    rc['enhanced_focal'] = {**emf, 'model': ef_obj}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"TRAINING ENHANCED ENSEMBLE WITH HIERARCHICAL METHOD\"); print(\"=\"*60)\n",
    "    eh_obj = AdvancedANNSVMEnsemble(em='hierarchical', uaf=True, ufl=True, vb=True)\n",
    "    eh_obj.fit(Xt, yt); emh = advanced_model_evaluation(eh_obj, Xte, yte)\n",
    "    rc['enhanced_hierarchical'] = {**emh, 'model': eh_obj}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80); print(\"COMPREHENSIVE COMPARISON ANALYSIS\"); print(\"=\"*80)\n",
    "    \n",
    "    cdf = pd.DataFrame({\n",
    "        'Original Ensemble': [rc['original']['accuracy'], 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'],\n",
    "        'Enhanced + Focal Loss': [rc['enhanced_focal']['accuracy'], rc['enhanced_focal']['balanced_accuracy'],\n",
    "                                 rc['enhanced_focal']['f1_weighted'], rc['enhanced_focal']['f1_macro'],\n",
    "                                 rc['enhanced_focal']['kappa'], rc['enhanced_focal']['mcc']],\n",
    "        'Enhanced + Hierarchical': [rc['enhanced_hierarchical']['accuracy'], rc['enhanced_hierarchical']['balanced_accuracy'],\n",
    "                                   rc['enhanced_hierarchical']['f1_weighted'], rc['enhanced_hierarchical']['f1_macro'],\n",
    "                                   rc['enhanced_hierarchical']['kappa'], rc['enhanced_hierarchical']['mcc']]\n",
    "    }, index=['Accuracy', 'Balanced Accuracy', 'F1-Weighted', 'F1-Macro', 'Cohen\\'s Kappa', 'MCC'])\n",
    "    \n",
    "    print(\"\\nPerformance Comparison:\"); print(cdf.round(4))\n",
    "    \n",
    "    fi = ((rc['enhanced_focal']['accuracy'] - rc['original']['accuracy']) / rc['original']['accuracy']) * 100\n",
    "    hi = ((rc['enhanced_hierarchical']['accuracy'] - rc['original']['accuracy']) / rc['original']['accuracy']) * 100\n",
    "    \n",
    "    print(f\"\\nImprovement Analysis:\"); print(f\"Enhanced + Focal Loss: {fi:.2f}% improvement\")\n",
    "    print(f\"Enhanced + Hierarchical: {hi:.2f}% improvement\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"GENERATING SEPARATE CONFUSION MATRIX PLOTS\"); print(\"=\"*60)\n",
    "    \n",
    "    mds = [('Original', rc['original']['model']), ('Enhanced + Focal', rc['enhanced_focal']['model']),\n",
    "           ('Enhanced + Hierarchical', rc['enhanced_hierarchical']['model'])]\n",
    "    acs = [rc['original']['accuracy'], rc['enhanced_focal']['accuracy'], rc['enhanced_hierarchical']['accuracy']]\n",
    "    \n",
    "    for idx, (nm, md) in enumerate(mds):\n",
    "        ypt = md.predict(Xte); ytet = md.le.transform(yte); cm_obj = cm(ytet, ypt)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6), dpi=600)\n",
    "        cls = [\"#ffffff\", \"#e0e0e0\", \"#c0c0c0\", \"#909090\", \"#606060\", \"#303030\"]\n",
    "        cmp = lscm.from_list(\"publication_cmap\", cls, N=256)\n",
    "        \n",
    "        sns.heatmap(cm_obj, annot=True, fmt='d', cmap=cmp, \n",
    "                   xticklabels=['High', 'Medium', 'Low'], yticklabels=['High', 'Medium', 'Low'],\n",
    "                   ax=ax, cbar=True, cbar_kws={'label': 'Count'})\n",
    "        \n",
    "        ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12); plt.tight_layout()\n",
    "        \n",
    "        fb = nm.lower().replace(' + ', '_').replace(' ', '_')\n",
    "        plt.savefig(os.path.join(pf, f'confusion_matrix_{fb}.pdf'), dpi=600, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(pf, f'confusion_matrix_{fb}.png'), dpi=600, bbox_inches='tight'); plt.show()\n",
    "        print(f\"Saved: {pf}/confusion_matrix_{fb}.pdf/.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"GENERATING SEPARATE ROC CURVE PLOTS\"); print(\"=\"*60)\n",
    "    \n",
    "    cmp_obj = {'High': '#800000', 'Medium': '#F0A202', 'Low': '#0D47A1'}\n",
    "    ls_obj = {'High': '-', 'Medium': '--', 'Low': '-.'}\n",
    "    \n",
    "    for idx, (nm, md) in enumerate(mds):\n",
    "        yppt = md.predict_proba(Xte); ytet = md.le.transform(yte)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), dpi=600)\n",
    "        \n",
    "        for i, cn in enumerate(['High', 'Medium', 'Low']):\n",
    "            yb = (ytet == i).astype(int); fpr, tpr, _ = rc(yb, yppt[:, i]); ra = auc_fn(fpr, tpr)\n",
    "            ax.plot(fpr, tpr, color=cmp_obj[cn], linestyle=ls_obj[cn], linewidth=3.0,\n",
    "                   label=f'{cn} Risk (AUC = {ra:.3f})', marker='o', markersize=4, markevery=0.1)\n",
    "        \n",
    "        ax.plot([0, 1], [0, 1], 'r--', alpha=0.6, linewidth=2, label='Random Classifier')\n",
    "        ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=12, frameon=True, fancybox=True, shadow=True, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12); plt.tight_layout()\n",
    "        \n",
    "        fb = nm.lower().replace(' + ', '_').replace(' ', '_')\n",
    "        plt.savefig(os.path.join(pf, f'roc_curve_{fb}.pdf'), dpi=600, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(pf, f'roc_curve_{fb}.png'), dpi=600, bbox_inches='tight'); plt.show()\n",
    "        print(f\"Saved: {pf}/roc_curve_{fb}.pdf/.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"GENERATING CLASS-SPECIFIC PERFORMANCE COMPARISON\"); print(\"=\"*60)\n",
    "    \n",
    "    cm_data = {}\n",
    "    for mn, md_data in rc.items():\n",
    "        if mn == 'original': continue\n",
    "        md_obj = md_data['model']; ypt = md_obj.predict(Xte); ytet = md_obj.le.transform(yte)\n",
    "        crp = cr(ytet, ypt, target_names=['High', 'Medium', 'Low'], output_dict=True)\n",
    "        cm_data[mn] = {'High': {'precision': crp['High']['precision'], 'recall': crp['High']['recall'], 'f1': crp['High']['f1-score']},\n",
    "                      'Medium': {'precision': crp['Medium']['precision'], 'recall': crp['Medium']['recall'], 'f1': crp['Medium']['f1-score']},\n",
    "                      'Low': {'precision': crp['Low']['precision'], 'recall': crp['Low']['recall'], 'f1': crp['Low']['f1-score']}}\n",
    "    \n",
    "    mts = ['precision', 'recall', 'f1']; cls_list = ['High', 'Medium', 'Low']\n",
    "    \n",
    "    for mt in mts:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6), dpi=600); x = np.arange(len(cls_list)); w = 0.25\n",
    "        cls_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        \n",
    "        for i, (mn, dt) in enumerate(cm_data.items()):\n",
    "            vs = [dt[cls][mt] for cls in cls_list]; lbl = mn.replace('_', ' ').title()\n",
    "            ax.bar(x + i * w, vs, w, label=lbl, color=cls_colors[i], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Risk Classes', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel(f'{mt.title()} Score', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x + w); ax.set_xticklabels(cls_list); ax.legend(fontsize=12); ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for i, (mn, dt) in enumerate(cm_data.items()):\n",
    "            vs = [dt[cls][mt] for cls in cls_list]\n",
    "            for j, v in enumerate(vs):\n",
    "                ax.text(j + i * w, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(pf, f'class_specific_{mt}_comparison.pdf'), dpi=600, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(pf, f'class_specific_{mt}_comparison.png'), dpi=600, bbox_inches='tight'); plt.show()\n",
    "        print(f\"Saved: {pf}/class_specific_{mt}_comparison.pdf/.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"FEATURE IMPORTANCE ANALYSIS\"); print(\"=\"*60)\n",
    "    \n",
    "    bmn = max(rc.keys(), key=lambda x: rc[x]['accuracy'] if 'accuracy' in rc[x] else 0)\n",
    "    bm = rc[bmn]['model']\n",
    "    \n",
    "    if hasattr(bm, 'rm') and bm.rm is not None:\n",
    "        print(\"Analyzing feature importance from Random Forest component...\")\n",
    "        bf = list(X.columns); is_obj = bm.rm.feature_importances_\n",
    "        if len(is_obj) > len(bf):\n",
    "            fn = bf + [f\"engineered_feature_{i}\" for i in range(len(is_obj) - len(bf))]\n",
    "        else: fn = bf[:len(is_obj)]\n",
    "        \n",
    "        idf = pd.DataFrame({'Feature': fn[:len(is_obj)], 'Importance': is_obj}).sort_values('Importance', ascending=False).head(15)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8), dpi=600)\n",
    "        brs = ax.barh(idf['Feature'], idf['Importance'], color='steelblue', alpha=0.8, edgecolor='navy', linewidth=1)\n",
    "        \n",
    "        for i, (br, vl) in enumerate(zip(brs, idf['Importance'])):\n",
    "            ax.text(vl + 0.001, br.get_y() + br.get_height()/2, f'{vl:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        ax.set_xlabel('Feature Importance Score', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Features', fontsize=14, fontweight='bold'); ax.grid(True, alpha=0.3, axis='x'); ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(pf, 'feature_importance_enhanced.pdf'), dpi=600, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(pf, 'feature_importance_enhanced.png'), dpi=600, bbox_inches='tight'); plt.show()\n",
    "        print(\"Top 10 Most Important Features:\"); print(idf.head(10).to_string(index=False))\n",
    "        print(f\"Saved: {pf}/feature_importance_enhanced.pdf/.png\")\n",
    "    else: print(\"Feature importance analysis not available\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60); print(\"SAVING COMPREHENSIVE RESULTS\"); print(\"=\"*60)\n",
    "    \n",
    "    cdf.to_excel('enhanced_ensemble_comparison.xlsx', engine='xlsxwriter')\n",
    "    dr = {'comparison_summary': cdf.to_dict(), 'improvements': {'focal_loss_improvement': fi, 'hierarchical_improvement': hi},\n",
    "          'best_model': bmn, 'best_accuracy': rc[bmn]['accuracy'], 'class_specific_metrics': cm_data}\n",
    "    \n",
    "    with open('enhanced_ensemble_detailed_results.json', 'w') as f: json.dump(dr, f, indent=2, default=str)\n",
    "    bm.save_models(f\"best_enhanced_ensemble_{bmn}\")\n",
    "    \n",
    "    print(\"Results saved:\"); print(\"- enhanced_ensemble_comparison.xlsx\")\n",
    "    print(\"- enhanced_ensemble_detailed_results.json\")\n",
    "    print(f\"- Individual confusion matrices: {pf}/confusion_matrix_[model_name].pdf/.png\")\n",
    "    print(f\"- Individual ROC curves: {pf}/roc_curve_[model_name].pdf/.png\")\n",
    "    print(f\"- Class-specific performance: {pf}/class_specific_[metric]_comparison.pdf/.png\")\n",
    "    print(f\"- {pf}/feature_importance_enhanced.pdf/.png\")\n",
    "    print(f\"- best_enhanced_ensemble_{bmn}_*.pkl/h5\")\n",
    "    \n",
    "    return rc, bmn\n",
    "\n",
    "def quick_improvement_suggestions():\n",
    "    sg = [{\"technique\": \"Class Weight Adjustment\", \"implementation\": \"Increase penalties for Low risk misclassification\", \"expected_improvement\": \"5-10%\", \"effort\": \"Low\"},\n",
    "         {\"technique\": \"Advanced Resampling\", \"implementation\": \"Use ADASYN or BorderlineSMOTE instead of standard SMOTE\", \"expected_improvement\": \"3-7%\", \"effort\": \"Low\"},\n",
    "         {\"technique\": \"Feature Engineering\", \"implementation\": \"Add interaction terms and polynomial features\", \"expected_improvement\": \"5-15%\", \"effort\": \"Medium\"},\n",
    "         {\"technique\": \"Ensemble Diversity\", \"implementation\": \"Add XGBoost or LightGBM to the ensemble\", \"expected_improvement\": \"8-12%\", \"effort\": \"Medium\"},\n",
    "         {\"technique\": \"Focal Loss Implementation\", \"implementation\": \"Fine-tune alpha and gamma parameters\", \"expected_improvement\": \"5-10%\", \"effort\": \"Low\"},\n",
    "         {\"technique\": \"Data Preprocessing\", \"implementation\": \"Use RobustScaler instead of StandardScaler\", \"expected_improvement\": \"2-5%\", \"effort\": \"Low\"}]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80); print(\"QUICK IMPROVEMENT SUGGESTIONS (PRIORITIZED)\"); print(\"=\"*80)\n",
    "    sgdf = pd.DataFrame(sg); print(sgdf.to_string(index=False)); return sg\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Python version: {tf.__version__}\"); print(f\"TensorFlow version: {tf.__version__}\"); print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting enhanced ensemble comparison...\"); rs, bm = run_enhanced_ensemble_comparison()\n",
    "        sg = quick_improvement_suggestions(); print_final_recommendations()\n",
    "        \n",
    "        print(f\"\\n{'='*80}\"); print(\"SUMMARY\"); print(f\"{'='*80}\")\n",
    "        print(f\"Best performing model: {bm}\"); print(f\"Best accuracy achieved: {rs[bm]['accuracy']:.4f}\")\n",
    "        \n",
    "        if 'original' in rs:\n",
    "            imp = ((rs[bm]['accuracy'] - rs['original']['accuracy']) / rs['original']['accuracy']) * 100\n",
    "            print(f\"Improvement over original: {imp:.2f}%\")\n",
    "        \n",
    "        print(\"\\nNext steps:\"); print(\"1. Review the generated comparison plots and reports\")\n",
    "        print(\"2. Implement the suggested quick wins\"); print(\"3. Consider domain-specific feature engineering\")\n",
    "        print(\"4. Test the best model on PHMSA data\"); print(\"5. Iterate based on real-world validation results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\"); import traceback; traceback.print_exc()\n",
    "        print(\"\\nTroubleshooting tips:\"); print(\"1. Check your data file path\")\n",
    "        print(\"2. Ensure all required packages are installed:\")\n",
    "        print(\"   pip install pandas numpy tensorflow scikit-learn imbalanced-learn matplotlib seaborn xlsxwriter joblib\")\n",
    "        print(\"3. Update scikit-learn if needed: pip install --upgrade scikit-learn\")\n",
    "        print(\"4. Reduce model complexity if memory issues occur\"); print(\"5. Try restarting your Python kernel/environment\")\n",
    "        \n",
    "        print(\"\\nAttempting to create sample plots...\")\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt, numpy as np\n",
    "            fig, ax = plt.subplots(figsize=(6, 4)); x = np.linspace(0, 10, 100); y = np.sin(x)\n",
    "            ax.plot(x, y); ax.set_xlabel('X values'); ax.set_ylabel('Y values'); plt.tight_layout()\n",
    "            plt.savefig(os.path.join(pf, 'test_plot.png'), dpi=300, bbox_inches='tight'); plt.show()\n",
    "            print(f\"Test plot saved to {pf}/test_plot.png\")\n",
    "        except Exception as pe:\n",
    "            print(f\"Even basic plotting failed: {pe}\"); print(\"There may be a fundamental issue with your Python environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec2f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
